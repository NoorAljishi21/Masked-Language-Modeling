{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f8c64f53",
      "metadata": {
        "id": "f8c64f53"
      },
      "source": [
        "# Fine-tuning a Model for Masked Language Modeling (MLM) Exam"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82276929",
      "metadata": {
        "id": "82276929"
      },
      "source": [
        "In this exam, you will be tasked with performing dataset preprocessing and fine-tuning a model for a masked language modeling task. Complete each step carefully according to the instructions provided.\n",
        "\n",
        "### Model and Dataset Information\n",
        "\n",
        "For this task, you will be working with the following:\n",
        "\n",
        "- **Model Checkpoint**: Use the pre-trained model checkpoint `bert-base-uncased` for both the model and tokenizer.\n",
        "- **Dataset**: You will be using the `CUTD/math_df` dataset. Ensure to load and preprocess the dataset correctly for training and evaluation.\n",
        "\n",
        "**Note:**\n",
        "- Any additional steps or methods you include that improve or enhance the results will be rewarded with bonus points if they are justified.\n",
        "- The steps outlined here are suggestions. You are free to implement alternative methods or approaches to achieve the task, as long as you explain the reasoning and the process at the bottom of the notebook.\n",
        "- You can use either TensorFlow or PyTorch for this task. If you prefer TensorFlow, feel free to use it when working with Hugging Face Transformers.\n",
        "- The number of data samples you choose to work with is flexible. However, if you select a very low number of samples and the training time is too short, this could affect the evaluation of your work."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca06b0e1",
      "metadata": {
        "id": "ca06b0e1"
      },
      "source": [
        "## Step 1: Load the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0d55afb",
      "metadata": {
        "id": "f0d55afb"
      },
      "source": [
        "Load the dataset and split it into training and test sets. Use 20% of the data for testing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "mygpc55TK7dL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "ff6d9898-7942-4d60-90ef-3f047bf38cae"
      },
      "id": "mygpc55TK7dL",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importting libraries"
      ],
      "metadata": {
        "id": "MC4wulRLL6Yv"
      },
      "id": "MC4wulRLL6Yv"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('english')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClaZHAvBMARi",
        "outputId": "44c3e96a-7c3e-4190-a7e8-1290a93aa480"
      },
      "id": "ClaZHAvBMARi",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Error loading english: Package 'english' not found in\n",
            "[nltk_data]     index\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "96a1c2ec",
      "metadata": {
        "id": "96a1c2ec"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"CUTD/math_df\", )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2m66ZlfLvuE",
        "outputId": "e4ddb57c-7aaf-449b-c9dd-d354f6956064"
      },
      "id": "u2m66ZlfLvuE",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['Unnamed: 0', 'text'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds= ds[\"train\"]"
      ],
      "metadata": {
        "id": "9x0VTl6RMjW8"
      },
      "id": "9x0VTl6RMjW8",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.remove_columns([\"Unnamed: 0\"])"
      ],
      "metadata": {
        "id": "ueB1OvmeMv1K"
      },
      "id": "ueB1OvmeMv1K",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "43edb50d",
      "metadata": {
        "id": "43edb50d"
      },
      "source": [
        "## Step 2: Load the Pretrained Model and Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fd0cc35",
      "metadata": {
        "id": "8fd0cc35"
      },
      "source": [
        "Use a pre-trained model and tokenizer for this task. Initialize both in this step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "bbaeda1e",
      "metadata": {
        "id": "bbaeda1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2399d9c-cef9-46a7-dd08-ddd52f286a34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9697936e",
      "metadata": {
        "id": "9697936e"
      },
      "source": [
        "## Step 3: Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff37bf02",
      "metadata": {
        "id": "ff37bf02"
      },
      "source": [
        "Define a preprocessing function that tokenizes the text data and prepares the inputs for the model. Ensure that you truncate the sequences to a maximum length of 512 tokens and pad them appropriately.\n",
        "\n",
        "**Bonus**: If you performed more comprehensive preprocessing, such as removing links, converting text to lowercase, or applying additional preprocessing techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "a7070fdb",
      "metadata": {
        "id": "a7070fdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2739ee2-7052-4a5c-cbd5-d0d81e7a1ef4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'A German literature college student who is interested in historical contexts of works and the personalities of authors, but normally dislikes theatrical works.'}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "ds[1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "punctuations = '''`÷×؛<>_()*&^%][.ـ،/:\"!?..,'{}~¦+|!”…“–ـ'''\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def text_cleaning(text):\n",
        "\n",
        "  # Removing links (URLs)\n",
        "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "  # Removing special characters and punctuations\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  pun = str.maketrans('', '', punctuations)\n",
        "  text = text.translate(pun)\n",
        "\n",
        "  # Lowercasing\n",
        "  text = text.lower()\n",
        "\n",
        "  # Removing English stopwords\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "\n",
        "  # Stemming\n",
        "  stemmed_words = []\n",
        "  for word in word_tokens:\n",
        "    stemmed_words.append(stemmer.stem(word))\n",
        "  stemmed_words = ' '.join(filtered_text)\n",
        "\n",
        "  return stemmed_words"
      ],
      "metadata": {
        "id": "cm1Wq17gN6Pf"
      },
      "id": "cm1Wq17gN6Pf",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= '''Natural language processing (NLP) is an interdisciplinary subfield of computer science and artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and'''"
      ],
      "metadata": {
        "id": "7NluC7PWSoHx"
      },
      "id": "7NluC7PWSoHx",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_cleaning(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "kNsY1No4SKYU",
        "outputId": "10474044-4cf1-4346-9082-0ca5fdd1fc1f"
      },
      "id": "kNsY1No4SKYU",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'natural language processing nlp interdisciplinary subfield computer science artificial intelligence primarily concerned providing computers ability process data encoded natural language thus closely related information retrieval knowledge representation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = ds.map(lambda x: {'text': text_cleaning(x['text'])})"
      ],
      "metadata": {
        "id": "lgjKAQ6EWymg"
      },
      "id": "lgjKAQ6EWymg",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Des5R2mNRUkw",
        "outputId": "af90a172-e706-4327-d939-23238d8afd36"
      },
      "id": "Des5R2mNRUkw",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'german literature college student interested historical contexts works personalities authors normally dislikes theatrical works'}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer([\" \".join(x) for x in examples[\"text\"]] , max_length=512, padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_ds = cleaned_text.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    remove_columns=cleaned_text.column_names,\n",
        ")"
      ],
      "metadata": {
        "id": "mP8LNrQBejOy"
      },
      "id": "mP8LNrQBejOy",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ds = tokenized_ds.train_test_split(test_size= 0.2)"
      ],
      "metadata": {
        "id": "n-MMUidWhA-P"
      },
      "id": "n-MMUidWhA-P",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tokenized_ds['train']\n",
        "val_ds =tokenized_ds['test']"
      ],
      "metadata": {
        "id": "ZoguYFr4f0dD"
      },
      "id": "ZoguYFr4f0dD",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46FdFTsPi3M8",
        "outputId": "bb9fdad4-24d5-4d00-d32d-66cf2e5d6b4a"
      },
      "id": "46FdFTsPi3M8",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
              "    num_rows: 8000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xET3XM6i5uW",
        "outputId": "4ea06a0e-6a22-41d6-e273-03d49526d9df"
      },
      "id": "1xET3XM6i5uW",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
              "    num_rows: 2000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71501d6c",
      "metadata": {
        "id": "71501d6c"
      },
      "source": [
        "## Step 4: Define Training Arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fe39714",
      "metadata": {
        "id": "8fe39714"
      },
      "source": [
        "Set up the training configuration, including parameters like learning rate, batch size, number of epochs, and weight decay."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "fJzSEKxIhNVa"
      },
      "id": "fJzSEKxIhNVa",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "7bcb9c19",
      "metadata": {
        "id": "7bcb9c19"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"our_model\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a6e70c9",
      "metadata": {
        "id": "5a6e70c9"
      },
      "source": [
        "## Step 5: Initialize the Trainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset = train_ds,\n",
        "    eval_dataset = val_ds,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "TaoFnjd7YLy9"
      },
      "id": "TaoFnjd7YLy9",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "27f30075",
      "metadata": {
        "id": "27f30075"
      },
      "source": [
        "Initialize the Trainer using the model, training arguments, and datasets (both training and evaluation)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc3a61df",
      "metadata": {
        "id": "dc3a61df"
      },
      "source": [
        "## Step 6: Fine-tune the Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "AkVgtKzAalgB",
        "outputId": "dcf1d410-7b37-4717-c786-4b5862f7af90"
      },
      "id": "AkVgtKzAalgB",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 16:22, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.485200</td>\n",
              "      <td>1.335223</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=1.5730953369140626, metrics={'train_runtime': 982.9487, 'train_samples_per_second': 8.139, 'train_steps_per_second': 1.017, 'total_flos': 2105638502400000.0, 'train_loss': 1.5730953369140626, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "fVz6rZgrkz3O",
        "outputId": "eb8fac0f-2bd9-4e4c-b216-c4c58e98b488"
      },
      "id": "fVz6rZgrkz3O",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 01:13]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 1.3439737558364868,\n",
              " 'eval_runtime': 73.3912,\n",
              " 'eval_samples_per_second': 27.251,\n",
              " 'eval_steps_per_second': 3.406,\n",
              " 'epoch': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "345e5004",
      "metadata": {
        "id": "345e5004"
      },
      "source": [
        "Run the training process using the initialized Trainer to fine-tune the model on the masked language modeling task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6e19dff",
      "metadata": {
        "id": "f6e19dff"
      },
      "source": [
        "## Step 7: Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ffc537d",
      "metadata": {
        "id": "9ffc537d"
      },
      "source": [
        "Use the fine-tuned model for inference. Create a pipeline for masked language modeling and test it with a sample sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "fdcda460",
      "metadata": {
        "id": "fdcda460",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f74057a-b554-4423-f119-76eb3368fa29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ourModel/tokenizer_config.json',\n",
              " 'ourModel/special_tokens_map.json',\n",
              " 'ourModel/vocab.txt',\n",
              " 'ourModel/added_tokens.json',\n",
              " 'ourModel/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "model.save_pretrained(\"ourModel\")\n",
        "tokenizer.save_pretrained(\"ourModel\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "MLM = pipeline(\"fill-mask\", model=\"/content/ourModel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZircrgomcn5",
        "outputId": "2867c77d-ca6c-42fa-a567-30294f51ca2e"
      },
      "id": "RZircrgomcn5",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MLM(\"Hello I'm a [MASK] model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH2DgiSbvB_D",
        "outputId": "2cdf4d10-94dd-4ba4-fc15-282f25b269f3"
      },
      "id": "yH2DgiSbvB_D",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.26639196276664734,\n",
              "  'token': 2535,\n",
              "  'token_str': 'role',\n",
              "  'sequence': \"hello i'm a role model.\"},\n",
              " {'score': 0.08713365346193314,\n",
              "  'token': 1039,\n",
              "  'token_str': 'c',\n",
              "  'sequence': \"hello i'm a c model.\"},\n",
              " {'score': 0.0815446600317955,\n",
              "  'token': 1038,\n",
              "  'token_str': 'b',\n",
              "  'sequence': \"hello i'm a b model.\"},\n",
              " {'score': 0.07847759872674942,\n",
              "  'token': 2047,\n",
              "  'token_str': 'new',\n",
              "  'sequence': \"hello i'm a new model.\"},\n",
              " {'score': 0.024538952857255936,\n",
              "  'token': 1050,\n",
              "  'token_str': 'n',\n",
              "  'sequence': \"hello i'm a n model.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MLM('T5 is a [MASK] bootcamp')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORUWwfXPu_-R",
        "outputId": "df360505-b192-4159-f778-af43070e4aa2"
      },
      "id": "ORUWwfXPu_-R",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.1572318822145462,\n",
              "  'token': 1040,\n",
              "  'token_str': 'd',\n",
              "  'sequence': 't5 is a d bootcamp'},\n",
              " {'score': 0.10642322152853012,\n",
              "  'token': 1048,\n",
              "  'token_str': 'l',\n",
              "  'sequence': 't5 is a l bootcamp'},\n",
              " {'score': 0.10559210926294327,\n",
              "  'token': 1056,\n",
              "  'token_str': 't',\n",
              "  'sequence': 't5 is a t bootcamp'},\n",
              " {'score': 0.09486913681030273,\n",
              "  'token': 1050,\n",
              "  'token_str': 'n',\n",
              "  'sequence': 't5 is a n bootcamp'},\n",
              " {'score': 0.0886613205075264,\n",
              "  'token': 1039,\n",
              "  'token_str': 'c',\n",
              "  'sequence': 't5 is a c bootcamp'}]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MLM('A food manufacturer aim to [MASK] consumer trust')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2K-nQZxvPFf",
        "outputId": "80dcd63e-9ee7-4d39-fda3-101302bf4439"
      },
      "id": "p2K-nQZxvPFf",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.1837501972913742,\n",
              "  'token': 1041,\n",
              "  'token_str': 'e',\n",
              "  'sequence': 'a food manufacturer aim to e consumer trust'},\n",
              " {'score': 0.14127621054649353,\n",
              "  'token': 1037,\n",
              "  'token_str': 'a',\n",
              "  'sequence': 'a food manufacturer aim to a consumer trust'},\n",
              " {'score': 0.07786485552787781,\n",
              "  'token': 1051,\n",
              "  'token_str': 'o',\n",
              "  'sequence': 'a food manufacturer aim to o consumer trust'},\n",
              " {'score': 0.05086292698979378,\n",
              "  'token': 1050,\n",
              "  'token_str': 'n',\n",
              "  'sequence': 'a food manufacturer aim to n consumer trust'},\n",
              " {'score': 0.04155458137392998,\n",
              "  'token': 1055,\n",
              "  'token_str': 's',\n",
              "  'sequence': 'a food manufacturer aim to s consumer trust'}]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sekQzhDCvgLR"
      },
      "id": "sekQzhDCvgLR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}